[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Fun time with my LLMs",
    "section": "",
    "text": "How would your AI make suggestions that are right specifically for you?\n\n\n\nPersonalization\n\nHCI\n\nCognitive Sciences\n\n\n\nWhat you might not encounter in conversations with a human\n\n\n\n\n\nSep 4, 2025\n\n\nFlorrie\n\n\n\n\n\n\n\n\n\n\n\n\nA test post\n\n\n\nLLMs\n\nprompting\n\nlogic\n\n\n\nAn example post from a Jupyter notebook\n\n\n\n\n\nFeb 2, 2024\n\n\nAn LLM User\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Blog1-book recommendation/A_new_post.html",
    "href": "posts/Blog1-book recommendation/A_new_post.html",
    "title": "How would your AI make suggestions that are right specifically for you?",
    "section": "",
    "text": "It’s the beginning of another beautiful semester — and according to the syllabus of my communication course, we are required to pick a book from a list of seven on the topic of AI to read throughout the academic term.\nUsually, I would let my large language model introduce each book to me to get a snapshot and then make a judgment myself. This time, I decided to take it a step further and treat the interaction as a small communicative experiment. I would ask the LLM to act as a consultant and actively suggest the best options for me.\nThis should be a reasonable task for an LLM: I’ve been using this particular model mainly for school and work since 2023, so our interaction history forms a rich data of my academic and intellectual interests. Therefore, it should be able to generate tailored suggestions. However,I decided to make my intention clear in the ask, because I don’t assume it will automatically take my background into account, which a human would probably do:\n\n\n\nImage\n\n\nHere, instead of merely paraphrasing the assignment, I included the full text from the syllabus. From a pragmatics perspective, this provides the LLM with maximum context for the current exchange (Shhh, not because I was too lazy).\nAfter 42 seconds, the model returned a long response structured in a very deliberate way: * An opening comment on the assignment. * His top 3 picks with rationales.\n\n\n\nImage\n\n\n\nA parting line, then a short introduction for each book.\nAnother parting line, then ideas for how the books could be incorporated into my existing work.\n\n\n\n\nImage\n\n\n\nA final parting line, the suggested ranking list\nTips for the assignment and, finally, a follow-up question to invite more interaction.\n\n\n\n\nImage\n\n\nThe response was comprehensive, seemingly designed to anticipate and address all my potential questions.\nWhile I found the response useful, it was excessively lengthy, especially considering the computational energy LLMs consume. I often have the sense that they favor exhaustiveness over the brevity we often value in human conversation, because talking is energy-consuming.\nBut here are the things I truly appreciated: 1. It provided a ready-to-use ranked list. The LLM correctly parsed the prompt’s explicit instructions (“rank your preferences”) and structured its primary output to meet that specific need.\n\nIt provided rationales connecting to my past work, which allowed me to critically evaluate the rationales and re-weigh my current prioritization.\nIt suggested practical applications. The ideas for incorporating the book’s content into my other projects were genuinely insightful.\n\nInterestingly, this final point highlights a key difference between human and AI communication. It’s difficult to imagine a human expert spontaneously generating such a detailed list of potential operational connections of each book with my past experience in a casual conversation. The tips for class would also be cognitively exausive to provide. While the cognitive load requried for these content would be too high for a human talker, they are easy-peasy for a LLM.\nSo, although the tones and expressions look humane, GPT’s very ability to perform this low-cost, high-output analysis in a causal converstion is actually inhuman, or unatural. It doesn’t rely on the same mental shortcuts as human do. To him, generating these chunks of tasks have no different from the other chunks, computationally."
  },
  {
    "objectID": "posts/Blog1-book recommendation/A_new_post.html#my-llms-sometimes-know-more-about-me-than-i-do",
    "href": "posts/Blog1-book recommendation/A_new_post.html#my-llms-sometimes-know-more-about-me-than-i-do",
    "title": "How would your AI make suggestions that are right specifically for you?",
    "section": "",
    "text": "It’s the beginning of another beautiful semester — and according to the syllabus of my communication course, we are required to pick a book from a list of seven on the topic of AI to read throughout the academic term.\nUsually, I would let my large language model introduce each book to me to get a snapshot and then make a judgment myself. This time, I decided to take it a step further and treat the interaction as a small communicative experiment. I would ask the LLM to act as a consultant and actively suggest the best options for me.\nThis should be a reasonable task for an LLM: I’ve been using this particular model mainly for school and work since 2023, so our interaction history forms a rich data of my academic and intellectual interests. Therefore, it should be able to generate tailored suggestions. However,I decided to make my intention clear in the ask, because I don’t assume it will automatically take my background into account, which a human would probably do:\n\n\n\nImage\n\n\nHere, instead of merely paraphrasing the assignment, I included the full text from the syllabus. From a pragmatics perspective, this provides the LLM with maximum context for the current exchange (Shhh, not because I was too lazy).\nAfter 42 seconds, the model returned a long response structured in a very deliberate way: * An opening comment on the assignment. * His top 3 picks with rationales.\n\n\n\nImage\n\n\n\nA parting line, then a short introduction for each book.\nAnother parting line, then ideas for how the books could be incorporated into my existing work.\n\n\n\n\nImage\n\n\n\nA final parting line, the suggested ranking list\nTips for the assignment and, finally, a follow-up question to invite more interaction.\n\n\n\n\nImage\n\n\nThe response was comprehensive, seemingly designed to anticipate and address all my potential questions.\nWhile I found the response useful, it was excessively lengthy, especially considering the computational energy LLMs consume. I often have the sense that they favor exhaustiveness over the brevity we often value in human conversation, because talking is energy-consuming.\nBut here are the things I truly appreciated: 1. It provided a ready-to-use ranked list. The LLM correctly parsed the prompt’s explicit instructions (“rank your preferences”) and structured its primary output to meet that specific need.\n\nIt provided rationales connecting to my past work, which allowed me to critically evaluate the rationales and re-weigh my current prioritization.\nIt suggested practical applications. The ideas for incorporating the book’s content into my other projects were genuinely insightful.\n\nInterestingly, this final point highlights a key difference between human and AI communication. It’s difficult to imagine a human expert spontaneously generating such a detailed list of potential operational connections of each book with my past experience in a casual conversation. The tips for class would also be cognitively exausive to provide. While the cognitive load requried for these content would be too high for a human talker, they are easy-peasy for a LLM.\nSo, although the tones and expressions look humane, GPT’s very ability to perform this low-cost, high-output analysis in a causal converstion is actually inhuman, or unatural. It doesn’t rely on the same mental shortcuts as human do. To him, generating these chunks of tasks have no different from the other chunks, computationally."
  },
  {
    "objectID": "posts/000_test_post/index.html",
    "href": "posts/000_test_post/index.html",
    "title": "A test post",
    "section": "",
    "text": "Read the following and see if you can answer the question:\n\n\nThere are three boxes in a stack. A pink one, a purple one and a green one. The pink one is in the middle of the stack with the purple below it. What is the color of the box on the bottom of the stack?\n\nMost likely you answered purple…\nBut it is possible you first said green (don’t worry if you did it is quite a common response!)\n\n\nIt’s a verbal visual problem that requires some deliberation and most likely for us to create a visual image of a stack of boxes as a reasoning aid.\nNotice also the complexity of the language used in the prompt to describe the scene. The use of natural language as opposed to a formal/logical calculus does hold the potential for ambiguity (multiple interpretations) and inexactness. However, it is the most normal/natural, efficient and broadly accessible way of communicating such information.\nNotice 1. The use of anaphor or reference. - Three BOXES -&gt; a pink/purple/green ONE as opposed to a pink box, a purple box and a green box. - The PINK ONE is in the middle ... the purple {BOX} below IT {the pink box}\nWe tend to try and avoid this kind of repetition to make our speech more coherent.\n\nAlthough the description in language may not be entirely concise, there is no real ambiguity as to how the boxes can be arranged. It must be: | Box | Stack position | | — | — | | GREEN | top | | PINK | middle | | PURPLE | bottom |\n\n\n\n\nGPT3.5 Turbo\n\nHad some difficulties and answered green\nLlama2 (chat 7B version)\n\n\nAnswered correctly (purple) and used some so-called chain-of-thought reasoning\n\nBing Chat (MS Co-pilot)\nThis LLM is GPT4 with some Microsoft specific tuning and RAG (search) additions.\nIt get’s it correct:\n\nThe color of the box on the bottom of the stack is purple. This is because the purple box is below the pink one, which is in the middle of the stack. Therefore, the purple box must be the lowest one. 🟪\n\nBut when you follow up and ask it to create visual representation of the box arrangement\n\nPROMPT: Can you create a visual representation of how the boxes are arranged?\n\nIt responds\n\nRESPONSE: Sure, I’ll try to create that. Here is a visual representation of how the boxes are arranged:\n\nAnd uses DALL-E3 to create 4 different attempts at a visual representation:\n\nNONE OF WHICH ARE CORRECT!!!"
  },
  {
    "objectID": "posts/000_test_post/index.html#a-visualization-problem-for-llms",
    "href": "posts/000_test_post/index.html#a-visualization-problem-for-llms",
    "title": "A test post",
    "section": "",
    "text": "Read the following and see if you can answer the question:\n\n\nThere are three boxes in a stack. A pink one, a purple one and a green one. The pink one is in the middle of the stack with the purple below it. What is the color of the box on the bottom of the stack?\n\nMost likely you answered purple…\nBut it is possible you first said green (don’t worry if you did it is quite a common response!)\n\n\nIt’s a verbal visual problem that requires some deliberation and most likely for us to create a visual image of a stack of boxes as a reasoning aid.\nNotice also the complexity of the language used in the prompt to describe the scene. The use of natural language as opposed to a formal/logical calculus does hold the potential for ambiguity (multiple interpretations) and inexactness. However, it is the most normal/natural, efficient and broadly accessible way of communicating such information.\nNotice 1. The use of anaphor or reference. - Three BOXES -&gt; a pink/purple/green ONE as opposed to a pink box, a purple box and a green box. - The PINK ONE is in the middle ... the purple {BOX} below IT {the pink box}\nWe tend to try and avoid this kind of repetition to make our speech more coherent.\n\nAlthough the description in language may not be entirely concise, there is no real ambiguity as to how the boxes can be arranged. It must be: | Box | Stack position | | — | — | | GREEN | top | | PINK | middle | | PURPLE | bottom |\n\n\n\n\nGPT3.5 Turbo\n\nHad some difficulties and answered green\nLlama2 (chat 7B version)\n\n\nAnswered correctly (purple) and used some so-called chain-of-thought reasoning\n\nBing Chat (MS Co-pilot)\nThis LLM is GPT4 with some Microsoft specific tuning and RAG (search) additions.\nIt get’s it correct:\n\nThe color of the box on the bottom of the stack is purple. This is because the purple box is below the pink one, which is in the middle of the stack. Therefore, the purple box must be the lowest one. 🟪\n\nBut when you follow up and ask it to create visual representation of the box arrangement\n\nPROMPT: Can you create a visual representation of how the boxes are arranged?\n\nIt responds\n\nRESPONSE: Sure, I’ll try to create that. Here is a visual representation of how the boxes are arranged:\n\nAnd uses DALL-E3 to create 4 different attempts at a visual representation:\n\nNONE OF WHICH ARE CORRECT!!!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This Blog collects some of my random thoughts on LLMs. Feel free to leave a message :)"
  }
]