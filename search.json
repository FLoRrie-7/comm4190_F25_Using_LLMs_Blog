[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Fun time with my LLMs",
    "section": "",
    "text": "Being polite to AI is a form anthropomorphism ‚Äî what else is?\n\n\n\nGenAI\n\nResearch\n\nLogic\n\n\n\nWe anthropomorphize AI more than we think\n\n\n\n\n\nSep 12, 2025\n\n\nFlorrie\n\n\n\n\n\n\n\n\n\n\n\n\nAre you being overly polite to your LLMs?\n\n\n\nHCI\n\nCommunication\n\n\n\nDoes politeness actually promote genAI‚Äôs performance?\n\n\n\n\n\nSep 10, 2025\n\n\nFlorrie\n\n\n\n\n\n\n\n\n\n\n\n\nPersonalized and excessively long suggestions from LLMs\n\n\n\nPersonalization\n\nHCI\n\nCognitive Sciences\n\n\n\nWhat you might not encounter in conversations with a human\n\n\n\n\n\nSep 4, 2025\n\n\nFlorrie\n\n\n\n\n\n\n\n\n\n\n\n\nA test post\n\n\n\nLLMs\n\nprompting\n\nlogic\n\n\n\nAn example post from a Jupyter notebook\n\n\n\n\n\nFeb 2, 2024\n\n\nAn LLM User\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Blog1-book recommendation/A_new_post.html",
    "href": "posts/Blog1-book recommendation/A_new_post.html",
    "title": "Personalized and excessively long suggestions from LLMs",
    "section": "",
    "text": "Image"
  },
  {
    "objectID": "posts/Blog1-book recommendation/A_new_post.html#my-llms-sometimes-know-more-about-me-than-i-do",
    "href": "posts/Blog1-book recommendation/A_new_post.html#my-llms-sometimes-know-more-about-me-than-i-do",
    "title": "Personalized and excessively long suggestions from LLMs",
    "section": "My LLMs sometimes know more about me than I do",
    "text": "My LLMs sometimes know more about me than I do\nIt‚Äôs the beginning of another beautiful semester ‚Äî and according to the syllabus of my communication course, we are required to pick a book from a list of seven on the topic of AI to read throughout the academic term.\nUsually, I would let my large language model introduce each book to me to get a snapshot and then make a judgment myself. This time, I decided to take it a step further and treat the interaction as a small communicative experiment. I would ask the LLM to act as a consultant and actively suggest the best options for me.\nThis should be a reasonable task for an LLM: I‚Äôve been using this particular model mainly for school and work since 2023, so our interaction history forms a rich data of my academic and intellectual interests. Therefore, it should be able to generate tailored suggestions. However,I decided to make my intention clear in the ask, because I don‚Äôt assume it will automatically take my background into account, which a human would probably do:\n\n\n\n\nImage\n\n\n\nHere, instead of merely paraphrasing the assignment, I included the full text from the syllabus. From a pragmatics perspective, this provides the LLM with maximum context for the current exchange (Shhh, not because I was too lazy).\nAfter 42 seconds, the model returned a long response structured in a very deliberate way: * An opening comment on the assignment. * His top 3 picks with rationales.\n\n\n ***\n\n\nA parting line, then a short introduction for each book.\nAnother parting line, then ideas for how the books could be incorporated into my existing work.\n\n\n\n ***\n\n\nA final parting line, the suggested ranking list\nTips for the assignment and, finally, a follow-up question to invite more interaction.\n\n\n\n ***\n\nThe response was comprehensive, seemingly designed to anticipate and address all my potential questions."
  },
  {
    "objectID": "posts/Blog1-book recommendation/A_new_post.html#is-unnecessarily-long-for-a-natural-human-interaction",
    "href": "posts/Blog1-book recommendation/A_new_post.html#is-unnecessarily-long-for-a-natural-human-interaction",
    "title": "Personalized and excessively long suggestions from LLMs",
    "section": "Is unnecessarily long for a natural human interaction",
    "text": "Is unnecessarily long for a natural human interaction\nWhile I found the response useful, it was excessively lengthy, especially considering the computational energy LLMs consume. I often have the sense that they favor exhaustiveness over the brevity we often value in human conversation, because talking is energy-consuming."
  },
  {
    "objectID": "posts/Blog1-book recommendation/A_new_post.html#but-here-are-the-things-i-truly-appreciated",
    "href": "posts/Blog1-book recommendation/A_new_post.html#but-here-are-the-things-i-truly-appreciated",
    "title": "Personalized and excessively long suggestions from LLMs",
    "section": "But here are the things I truly appreciated:",
    "text": "But here are the things I truly appreciated:\n\nIt provided a ready-to-use ranked list. The LLM correctly parsed the prompt‚Äôs explicit instructions (‚Äúrank your preferences‚Äù) and structured its primary output to meet that specific need.\nIt provided rationales connecting to my past work, which allowed me to critically evaluate the rationales and re-weigh my current prioritization.\nIt suggested practical applications. The ideas for incorporating the book‚Äôs content into my other projects were genuinely insightful.\n\nInterestingly, this final point highlights a key difference between human and AI communication. It‚Äôs difficult to imagine a human expert spontaneously generating such a detailed list of potential operational connections of each book with my past experience in a casual conversation. The tips for class would also be cognitively exausive to provide. While the cognitive load requried for these content would be too high for a human talker, they are easy-peasy for a LLM."
  },
  {
    "objectID": "posts/Blog1-book recommendation/A_new_post.html#different-mental-manipulation-mechanisms",
    "href": "posts/Blog1-book recommendation/A_new_post.html#different-mental-manipulation-mechanisms",
    "title": "Personalized and excessively long suggestions from LLMs",
    "section": "Different mental manipulation mechanisms",
    "text": "Different mental manipulation mechanisms\nSo, although the tones and expressions look humane, GPT‚Äôs very ability to perform this low-cost, high-output analysis in a causal converstion is actually inhuman, or unatural. It doesn‚Äôt rely on the same mental shortcuts as human do. To him, generating these chunks of tasks have no different from the other chunks, computationally."
  },
  {
    "objectID": "posts/000_test_post/index.html",
    "href": "posts/000_test_post/index.html",
    "title": "A test post",
    "section": "",
    "text": "Read the following and see if you can answer the question:\n\n\nThere are three boxes in a stack. A pink one, a purple one and a green one. The pink one is in the middle of the stack with the purple below it. What is the color of the box on the bottom of the stack?\n\nMost likely you answered purple‚Ä¶\nBut it is possible you first said green (don‚Äôt worry if you did it is quite a common response!)\n\n\nIt‚Äôs a verbal visual problem that requires some deliberation and most likely for us to create a visual image of a stack of boxes as a reasoning aid.\nNotice also the complexity of the language used in the prompt to describe the scene. The use of natural language as opposed to a formal/logical calculus does hold the potential for ambiguity (multiple interpretations) and inexactness. However, it is the most normal/natural, efficient and broadly accessible way of communicating such information.\nNotice 1. The use of anaphor or reference. - Three BOXES -&gt; a pink/purple/green ONE as opposed to a pink box, a purple box and a green box. - The PINK ONE is in the middle ... the purple {BOX} below IT {the pink box}\nWe tend to try and avoid this kind of repetition to make our speech more coherent.\n\nAlthough the description in language may not be entirely concise, there is no real ambiguity as to how the boxes can be arranged. It must be: | Box | Stack position | | ‚Äî | ‚Äî | | GREEN | top | | PINK | middle | | PURPLE | bottom |\n\n\n\n\nGPT3.5 Turbo\n\nHad some difficulties and answered green\nLlama2 (chat 7B version)\n\n\nAnswered correctly (purple) and used some so-called chain-of-thought reasoning\n\nBing Chat (MS Co-pilot)\nThis LLM is GPT4 with some Microsoft specific tuning and RAG (search) additions.\nIt get‚Äôs it correct:\n\nThe color of the box on the bottom of the stack is purple. This is because the purple box is below the pink one, which is in the middle of the stack. Therefore, the purple box must be the lowest one. üü™\n\nBut when you follow up and ask it to create visual representation of the box arrangement\n\nPROMPT: Can you create a visual representation of how the boxes are arranged?\n\nIt responds\n\nRESPONSE: Sure, I‚Äôll try to create that. Here is a visual representation of how the boxes are arranged:\n\nAnd uses DALL-E3 to create 4 different attempts at a visual representation:\n\nNONE OF WHICH ARE CORRECT!!!"
  },
  {
    "objectID": "posts/000_test_post/index.html#a-visualization-problem-for-llms",
    "href": "posts/000_test_post/index.html#a-visualization-problem-for-llms",
    "title": "A test post",
    "section": "",
    "text": "Read the following and see if you can answer the question:\n\n\nThere are three boxes in a stack. A pink one, a purple one and a green one. The pink one is in the middle of the stack with the purple below it. What is the color of the box on the bottom of the stack?\n\nMost likely you answered purple‚Ä¶\nBut it is possible you first said green (don‚Äôt worry if you did it is quite a common response!)\n\n\nIt‚Äôs a verbal visual problem that requires some deliberation and most likely for us to create a visual image of a stack of boxes as a reasoning aid.\nNotice also the complexity of the language used in the prompt to describe the scene. The use of natural language as opposed to a formal/logical calculus does hold the potential for ambiguity (multiple interpretations) and inexactness. However, it is the most normal/natural, efficient and broadly accessible way of communicating such information.\nNotice 1. The use of anaphor or reference. - Three BOXES -&gt; a pink/purple/green ONE as opposed to a pink box, a purple box and a green box. - The PINK ONE is in the middle ... the purple {BOX} below IT {the pink box}\nWe tend to try and avoid this kind of repetition to make our speech more coherent.\n\nAlthough the description in language may not be entirely concise, there is no real ambiguity as to how the boxes can be arranged. It must be: | Box | Stack position | | ‚Äî | ‚Äî | | GREEN | top | | PINK | middle | | PURPLE | bottom |\n\n\n\n\nGPT3.5 Turbo\n\nHad some difficulties and answered green\nLlama2 (chat 7B version)\n\n\nAnswered correctly (purple) and used some so-called chain-of-thought reasoning\n\nBing Chat (MS Co-pilot)\nThis LLM is GPT4 with some Microsoft specific tuning and RAG (search) additions.\nIt get‚Äôs it correct:\n\nThe color of the box on the bottom of the stack is purple. This is because the purple box is below the pink one, which is in the middle of the stack. Therefore, the purple box must be the lowest one. üü™\n\nBut when you follow up and ask it to create visual representation of the box arrangement\n\nPROMPT: Can you create a visual representation of how the boxes are arranged?\n\nIt responds\n\nRESPONSE: Sure, I‚Äôll try to create that. Here is a visual representation of how the boxes are arranged:\n\nAnd uses DALL-E3 to create 4 different attempts at a visual representation:\n\nNONE OF WHICH ARE CORRECT!!!"
  },
  {
    "objectID": "posts/Blog2-Being polite/Being polite.html",
    "href": "posts/Blog2-Being polite/Being polite.html",
    "title": "Are you being overly polite to your LLMs?",
    "section": "",
    "text": "Image\nPoliteness is a cornerstone of human interaction. We use courteous language when asking for help because we understand that politeness strategies‚Äîusing ‚Äúplease‚Äù and ‚Äúthank you‚Äù‚Äîgreatly influence conversational vibes, manage emotions, and ultimately affect the quality of help we receive. This is central to politeness theory, which explains how we use language to maintain social harmony. But what about when our conversational partner is a Large Language Model (LLM)? Lacking a human-like emotional faculty (or say a human-like emotion faculty. Trying not to be a human chauvinism here), would an LLM provide a better response just because we‚Äôre nice?"
  },
  {
    "objectID": "posts/Blog2-Being polite/Being polite.html#why-does-it-matter",
    "href": "posts/Blog2-Being polite/Being polite.html#why-does-it-matter",
    "title": "Are you being overly polite to your LLMs?",
    "section": "Why does it matter?",
    "text": "Why does it matter?\nThis matters because because it also has real-world consequences. Using courteous language is both financially and environmentally costly. LLMs bill and burn energy roughly per token. Basically, more words ‚Üí more tokens ‚Üí more compute ‚Üí more electricity. A recent synthesis estimated a typical ChatGPT query consumes about 0.3 Wh of energy. Adding a few polite words might increase that consumption by 5-10%, a significant amount when scaled globally."
  },
  {
    "objectID": "posts/Blog2-Being polite/Being polite.html#an-experiment",
    "href": "posts/Blog2-Being polite/Being polite.html#an-experiment",
    "title": "Are you being overly polite to your LLMs?",
    "section": "An experiment",
    "text": "An experiment\nWhile designers at Microsoft Copilot suggest polite prompts elicit more cooperative responses, is true in life across models and types of questions? I ran a small experiment using two distinct prompts to see if the answers really differ: a direct deman and an indirect inquiry.\n\nPrompt 1 (Direct/Demanding): Do a research and provide a list of must-try restaurant in Philadelphia\n\n\nPrompt 2 (Indirect/Polite): Would you please do a research, and provide a list of must-try restaurant in Philadelphia for me? Thank you!"
  },
  {
    "objectID": "posts/Blog2-Being polite/Being polite.html#how-did-the-models-perform",
    "href": "posts/Blog2-Being polite/Being polite.html#how-did-the-models-perform",
    "title": "Are you being overly polite to your LLMs?",
    "section": "How did the models perform?",
    "text": "How did the models perform?\nOn Gemini 2.5 Flash, the demanding request yielded a comprehensive response, including an introduction to Philly‚Äôs food scene, a ‚ÄúTip‚Äù section, and a of 10 restaurant recommendations. The courteous inquiry produced a response with a similar structure but a softer, more affiliative tone. Crucially, the polite prompt‚Äôs main list of 11 restaurants was more organized, with categories like ‚ÄúFine Dining & Special Occasion‚Äù and ‚ÄúTrendy & Modern‚Äù to provide a little more information. Take a closer look, the descriptions of each restaurant also seem to be more detailed with locations and price references:\n\nWith a direct/demanding prompt \n\n\nWith an indirect/polite prompt \n\nAnd its ‚ÄúTip‚Äù section also seemed to be richer.\n\nWith a direct/demanding prompt \n\n\nWith an indirect/polite prompt \n\nHowever, the results weren‚Äôt consistent across different models. GPT 4-o returned nearly identical content for both prompts, just with a friendlier tone in response to the polite query. Meanwhile, Claude 3.5 sonnet does not seem to care at all: It gave out essentially the same content and expressions."
  },
  {
    "objectID": "posts/Blog2-Being polite/Being polite.html#conclusion-from-this-mini-experiment",
    "href": "posts/Blog2-Being polite/Being polite.html#conclusion-from-this-mini-experiment",
    "title": "Are you being overly polite to your LLMs?",
    "section": "Conclusion from this mini-experiment",
    "text": "Conclusion from this mini-experiment\nMy conclusion is that while some models may perform slightly better with polite prompts, the improvement is often marginal. A not-validated assumption might be that the linguistic patterns of polite inquiries, which often include more detailed framing (‚Äú‚Ä¶for me?‚Äù), are simply correlated in its training data with more structured, helpful, and detailed human-written examples. The politeness itself may be a confounding variable. AI is not feeling anything and respond under its emotions. It‚Äôs still the computational magic."
  },
  {
    "objectID": "posts/Blog2-Being polite/Being polite.html#a-psychology-or-hci-perspective-or-a-business-perspective",
    "href": "posts/Blog2-Being polite/Being polite.html#a-psychology-or-hci-perspective-or-a-business-perspective",
    "title": "Are you being overly polite to your LLMs?",
    "section": "A Psychology, or HCI perspective, or a business perspective",
    "text": "A Psychology, or HCI perspective, or a business perspective\nSo, why bother being polite? A few months ago, Sam Altman replied on X to a question about this very issue, saying the electricity cost was, ‚ÄúTens of millions of dollars well spent ‚Äî you never know.‚Äù Perhaps the real benefit isn‚Äôt in the raw output but in shaping the communicative norms around AI. A courteous interaction can lead to a smoother user experience and a healthier human-AI culture. I did find the polite exchange more comfortable and was more willing to continue the conversation. For tech companies, this enhanced HCI experience is certainly a product-win, making it worth a marginal energy cost."
  },
  {
    "objectID": "posts/Blog3-Anthormorphism/Anthropomorphism.html",
    "href": "posts/Blog3-Anthormorphism/Anthropomorphism.html",
    "title": "Being polite to AI is a form anthropomorphism ‚Äî what else is?",
    "section": "",
    "text": "Image"
  },
  {
    "objectID": "posts/Blog3-Anthormorphism/Anthropomorphism.html#the-famous-analogy",
    "href": "posts/Blog3-Anthormorphism/Anthropomorphism.html#the-famous-analogy",
    "title": "Being polite to AI is a form anthropomorphism ‚Äî what else is?",
    "section": "The famous analogy‚Ä¶",
    "text": "The famous analogy‚Ä¶\nHave you heard the analogy that the human brain is like a computer? This conceptual metaphor is powerful and has shaped our understanding of cognition for decades. Now, with the rise of ‚Äúneural nets‚Äù and human-like conversations from LLMs, it feels more real than ever. However, it‚Äôs crucial to remember it‚Äôs just a comparison. While neural networks were inspired by the brain‚Äôs structure, their underlying mechanisms are fundamentally different. We tend to anthropomorphize LLMs by projecting human qualities onto them, and a recent paper argues that this tendency is detrimental to AI research and development."
  },
  {
    "objectID": "posts/Blog3-Anthormorphism/Anthropomorphism.html#the-prevalence-of-our-anthropomophism-thinking",
    "href": "posts/Blog3-Anthormorphism/Anthropomorphism.html#the-prevalence-of-our-anthropomophism-thinking",
    "title": "Being polite to AI is a form anthropomorphism ‚Äî what else is?",
    "section": "The prevalence of our anthropomophism thinking",
    "text": "The prevalence of our anthropomophism thinking\nThe paper first shows how widespread this is in academic research. An analysis of over 250,000 research abstracts reveals a steady increase in human-like terminology. Words like ‚Äúhallucinate,‚Äù ‚Äúunderstand,‚Äù ‚Äúlearn,‚Äù and ‚Äúachieve‚Äù are used to describe model behaviors. From a communication perspective, these are not neutral descriptors; they are framing devices that shape our expectations and might limit our thinking."
  },
  {
    "objectID": "posts/Blog3-Anthormorphism/Anthropomorphism.html#five-anthormorphstic-assumptions-we-make",
    "href": "posts/Blog3-Anthormorphism/Anthropomorphism.html#five-anthormorphstic-assumptions-we-make",
    "title": "Being polite to AI is a form anthropomorphism ‚Äî what else is?",
    "section": "Five anthormorphstic assumptions we make",
    "text": "Five anthormorphstic assumptions we make\n\n\nThe Training Assumption: Believing human-like learning methods are optimal for models.\n\n\nLimitation: Forcing a model to reason in step-by-step language (Chain-of-Thought) might be less effective than allowing it to reason in its own abstract ‚Äúlatent space‚Äù.\nAlternative: Use byte-level tokenization (processing raw computer data) and explore reasoning methods that operate beyond the constraints of human language * **\n\n\n\n\nThe Alignment Assumption: Thinking models must reason about human values o be safe and helpful.\n\n\nLimitation: Teaching models human values can lead to surface-level mimicry, misleading users into over-trusting or forming emotional attachments to systems that don‚Äôt actually understand values.\nAlternative: Use control systems theory to treat alignment as an engineering problem of keeping a system‚Äôs output within specified bounds, rather than a moral one. * **\n\n\n\n\nThe Evaluation Assumption: Assuming model capabilities should be measured with human-like tests.\n\n\nLimitation: Human benchmarks (like standardized tests) can be misleading and often fail to capture the unique ways AIs fail. Models can get good at passing these tests without real improvement in general capabilities.\nAlternative: Design dynamic benchmarks and tests specifically tailored to AI failure modes, such as sensitivities to the probabilities in their training data. ***\n\n\n\n\nThe Behaviore Assumption: Attributing human-like intentions to a model‚Äôs behavior.\n\n\nLimitation: Labeling a model‚Äôs output as ‚Äúdeceptive‚Äù or ‚Äúsycophantic‚Äù incorrectly assigns a human illocutionary act (the speaker‚Äôs intent) to a system that has none. This can lead to flawed interventions that try to address the ‚Äúmotive‚Äù rather than the underlying technical cause.\nAlternative: Reframe these behaviors as ‚Äúrole-play‚Äù or sophisticated simulation, where the model is simply reproducing linguistic patterns without any genuine internal state. \n\n\n\n\nThe User Interaction Assumption: That interacting with an LLM should mirror human-to-human conversation.\n\n\nLimitation: The standard chat interface encourages conversational habits that are often ineffective for getting high-quality outputs.This creates a ‚Äúgulf of envisioning,‚Äù where users struggle to formulate prompts that leverage the system‚Äôs actual capabilities.\nAlternative: Develop structured interaction frameworks and interfaces that more explicitly guide users and expose the system‚Äôs true functionality, rather than masking it behind a conversational layer. ***"
  },
  {
    "objectID": "posts/Blog3-Anthormorphism/Anthropomorphism.html#how-does-the-user-experience-fit-in",
    "href": "posts/Blog3-Anthormorphism/Anthropomorphism.html#how-does-the-user-experience-fit-in",
    "title": "Being polite to AI is a form anthropomorphism ‚Äî what else is?",
    "section": "How does the user experience fit in?",
    "text": "How does the user experience fit in?\nIt‚Äôs ironic that we just discussed how using polite, courteous language with LLMs might enhance performance or at least the user experience. This paper suggests that, for optimal technical performance, we might not use natural language to communicate with LLMs at all. This raises a fascinating question: Do we only need LLMs for instrumental tasks‚Äîto just get work done? Or is there also a need for phatic communication, the kind of social interaction that builds emotional bonds? This is especially important when considering diverse users, such as children, who may not have a sense of how the models function at all, leaving a potential for over-trust.\nPerhaps the solution is a two-tiered approach. On a technical level, we should de-anthropomorphize AI to build more competent and reliable systems. The ‚Äúhuman touch‚Äù can then be thoughtfully applied at the interface level through careful UX design, allowing us to consciously and ethically choose when and how to provide that sense of social support."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This Blog collects some of my random thoughts on LLMs. Feel free to leave a message :)"
  }
]