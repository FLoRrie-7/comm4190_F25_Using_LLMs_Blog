{
 "cells": [
  {
   "cell_type": "raw",
   "id": "6e5a2971-c936-404c-9075-0f365f8362a6",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Being polite to AI is a form anthropomorphism â€” what else is?\"\n",
    "description: \"We anthropomorphize AI more than we think\" \n",
    "author: \"Florrie\"\n",
    "date: \"9/12/2025\"\n",
    "categories:\n",
    "  - GenAI\n",
    "  - Research\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c76905-74ca-4045-b835-274a41f8db41",
   "metadata": {},
   "source": [
    "                                 ![Image](cat.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebbd1f6-7b07-4d36-b4a1-9a07eda3b79d",
   "metadata": {},
   "source": [
    "Have you heard the analogy that the human brain is like a computer? This **conceptual metaphor** is powerful and has shaped our understanding of cognition for decades. Now, with the rise of \"neural nets\" and human-like conversations from LLMs, it feels more real than ever. However, it's crucial to remember it's just a comparison. While neural networks were inspired by the brain's structure, their underlying mechanisms are fundamentally different. We tend to anthropomorphize LLMs by projecting human qualities onto them, and a recent paper argues that this tendency is __detrimental__ to AI research and development.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb342eb9-6344-4990-a44c-b3c99b094d69",
   "metadata": {},
   "source": [
    "## The prevalence of our anthropomophism thinking\n",
    "The paper first shows how widespread this is in academic research. An analysis of over 250,000 research abstracts reveals a steady increase in human-like terminology. Words like \"hallucinate,\" \"understand,\" \"learn,\" and \"achieve\" are used to describe model behaviors. From a communication perspective, these are not neutral descriptors; they are framing devices that shape our expectations and might limit our thinking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d74152d-9300-41f6-a546-93bfbf1bb9a7",
   "metadata": {},
   "source": [
    "## Five anthormorphstic assumptions we make\n",
    "> 1. __The Training Assumption__: Believing human-like learning methods are optimal for models.\n",
    "> * Limitation: Forcing a model to reason in step-by-step language (Chain-of-Thought) might be less effective than allowing it to reason in its own abstract \"latent space\".\n",
    "> * Alternative: Use byte-level tokenization (processing raw computer data) and explore reasoning methods that operate beyond the constraints of human language\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2cb152-eadd-4df3-816f-1029073b4901",
   "metadata": {},
   "source": [
    "> 1. __The Training Assumption__: Believing human-like learning methods are optimal for models.\n",
    "> * Limitation: Forcing a model to reason in step-by-step language (Chain-of-Thought) might be less effective than allowing it to reason in its own abstract \"latent space\".\n",
    "> * Alternative: Use byte-level tokenization (processing raw computer data) and explore reasoning methods that operate beyond the constraints of human language"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3-12 (COMM4190)",
   "language": "python",
   "name": "python3-12_comm4190"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
