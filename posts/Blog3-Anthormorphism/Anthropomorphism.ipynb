{
 "cells": [
  {
   "cell_type": "raw",
   "id": "6e5a2971-c936-404c-9075-0f365f8362a6",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Being polite to AI is a form anthropomorphism — what else is?\"\n",
    "description: \"We anthropomorphize AI more than we think\" \n",
    "author: \"Florrie\"\n",
    "date: \"9/12/2025\"\n",
    "categories:\n",
    "  - GenAI\n",
    "  - Research\n",
    "  - Logic\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c76905-74ca-4045-b835-274a41f8db41",
   "metadata": {},
   "source": [
    "![Image](cat.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebbd1f6-7b07-4d36-b4a1-9a07eda3b79d",
   "metadata": {},
   "source": [
    "## The famous analogy...\n",
    "Have you heard the analogy that the human brain is like a computer? This **conceptual metaphor** is powerful and has shaped our understanding of cognition for decades. Now, with the rise of \"neural nets\" and human-like conversations from LLMs, it feels more real than ever. However, it's crucial to remember **it's just a comparison**. While neural networks were inspired by the brain's structure, their underlying mechanisms are fundamentally different. We tend to anthropomorphize LLMs by projecting human qualities onto them, and a recent paper argues that this tendency is __detrimental__ to AI research and development.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb342eb9-6344-4990-a44c-b3c99b094d69",
   "metadata": {},
   "source": [
    "## The prevalence of our anthropomophism thinking\n",
    "The paper first shows how widespread this is in academic research. An analysis of over 250,000 research abstracts reveals a steady increase in human-like terminology. Words like \"hallucinate,\" \"understand,\" \"learn,\" and \"achieve\" are used to describe model behaviors. From a communication perspective, these are not neutral descriptors; they are framing devices that shape our expectations and might limit our thinking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d74152d-9300-41f6-a546-93bfbf1bb9a7",
   "metadata": {},
   "source": [
    "## Five anthormorphstic assumptions we make\n",
    "> 1. __The Training Assumption__: Believing human-like learning methods are optimal for models.\n",
    "> * Limitation: Forcing a model to reason in step-by-step language (Chain-of-Thought) might be less effective than allowing it to reason in its own abstract \"latent space\".\n",
    "> * Alternative: Use byte-level tokenization (processing raw computer data) and explore reasoning methods that operate beyond the constraints of human language\n",
    "> * **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2cb152-eadd-4df3-816f-1029073b4901",
   "metadata": {},
   "source": [
    "> 2. __The Alignment Assumption__: Thinking models must reason about human values o be safe and helpful.\n",
    "> * Limitation: Teaching models human values can lead to surface-level mimicry, misleading users into over-trusting or forming emotional attachments to systems that don’t actually understand values.\n",
    "> * Alternative: Use control systems theory to treat alignment as an engineering problem of keeping a system's output within specified bounds, rather than a moral one.\n",
    "> * **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6519917c-e7ef-49d2-ae44-fbfa2cfeda3d",
   "metadata": {},
   "source": [
    "> 3. __The Evaluation Assumption__: Assuming model capabilities should be measured with human-like tests.\n",
    "> * Limitation: Human benchmarks (like standardized tests) can be misleading and often fail to capture the unique ways AIs fail. Models can get good at passing these tests without real improvement in general capabilities.\n",
    "> * Alternative: Design dynamic benchmarks and tests specifically tailored to AI failure modes, such as sensitivities to the probabilities in their training data.\n",
    "> ***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5beaf0c-8299-47dc-bf98-24b5d841318f",
   "metadata": {},
   "source": [
    "> 4. __The Behaviore Assumption__: Attributing human-like intentions to a model’s behavior.\n",
    "> * Limitation: Labeling a model's output as \"deceptive\" or \"sycophantic\" incorrectly assigns a human illocutionary act (the speaker's intent) to a system that has none. This can lead to flawed interventions that try to address the \"motive\" rather than the underlying technical cause.\n",
    "> * Alternative: Reframe these behaviors as \"role-play\" or sophisticated simulation, where the model is simply reproducing linguistic patterns without any genuine internal state.\n",
    "![Image](Lying_AI.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0c652ce4-5330-4f95-904f-ae192791f4c6",
   "metadata": {},
   "source": [
    "> 5. __The User Interaction Assumption__: That interacting with an LLM should mirror human-to-human conversation.\n",
    "> * Limitation: The standard chat interface encourages conversational habits that are often ineffective for getting high-quality outputs.This creates a \"gulf of envisioning,\" where users struggle to formulate prompts that leverage the system's actual capabilities.\n",
    "> * Alternative: Develop structured interaction frameworks and interfaces that more explicitly guide users and expose the system's true functionality, rather than masking it behind a conversational layer.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0343447-89ca-4c6d-9cac-d7bd70119d87",
   "metadata": {},
   "source": [
    "## How does the user experience fit in?\n",
    "It’s ironic that we just discussed how using polite, courteous language with LLMs might enhance performance or at least the user experience. This paper suggests that, for optimal technical performance, we might not use natural language to communicate with LLMs at all. This raises a fascinating question: **Do we only need LLMs for instrumental tasks—to just get work done? Or is there also a need for phatic communication, the kind of social interaction that builds emotional bonds?** This is especially important when considering diverse users, such as children, who may not have a sense of how the models function at all, leaving a potential for over-trust.\n",
    "\n",
    "Perhaps the solution is a two-tiered approach. On a technical level, we should de-anthropomorphize AI to build more competent and reliable systems. The \"human touch\" can then be thoughtfully applied at the **interface level** through careful UX design, allowing us to consciously and ethically **choose when and how** to provide that sense of social support.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3-12 (COMM4190)",
   "language": "python",
   "name": "python3-12_comm4190"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
